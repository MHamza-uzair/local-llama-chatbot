# Local LLaMA Chatbot

A beginner-friendly **local chatbot** using **LLaMA 3 via Ollama**, **FastAPI backend**, and a **responsive HTML/CSS/JS frontend**.

This project lets you **chat with a local LLM on your machine** while learning:

How to use **Ollama for local LLaMA 3 models**  
 Building a **FastAPI backend** to handle requests  
 Connecting a **frontend chat interface** to a backend  
 Managing conversation history  
 Switching between **light/dark mode**

---

## Features

- Local **LLaMA 3** chatbot, no API key costs
- **FastAPI backend** for structured API endpoints
- Simple, clean **HTML/CSS/JS frontend**
- **Conversation history** stored locally
- Light and Dark Mode toggle
- **Clear button** to reset chat history

---

## üõ†Ô∏è Tech Stack

- Python, FastAPI
- Ollama (for local LLaMA 3 models)
- HTML, CSS, JavaScript

---

## Setup Instructions

1Ô∏è- **Install Ollama & pull a model:**

```bash
ollama pull llama3:8b
ollama serve
```
